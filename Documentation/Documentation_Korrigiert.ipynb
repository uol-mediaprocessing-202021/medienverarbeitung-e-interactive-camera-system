{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnKkxP6Qhodc"
      },
      "source": [
        "#Motivation\n",
        "Due to the COVID-19 pandemic, many people are tied to their home offices and subsequently have to do their work often with meager equipment. Moreover, many students are also increasingly exposed to online tutoring sessions, in which for example they have to present solutions for different exercises. In some cases, students may even be the tutor and are leading the lessons. Under normal circumstances, one would sit in a room with a projector and blackboard or whiteboard. The latter two media would help to explain any questions by visually supporting what is being said. However, this technique is not easy to implement at home. It would require technical equipment such as a graphics tablet or a document camera. One of many problem is that not everyone has access to such technology and therefore the quality of the tutoring session could potentially suffer. It can also be difficult for someone who has acces to them but not the means to understand how to use them properly.To counteract such difficulties, an interactive camera system can be presented as an attractive solution. \n",
        "\n",
        "As a possible solution to the problems that have been named, we would like to introduce our approach to an interactive camera within an active camera system. \n",
        "For our approach to be succesful,two goals have to be meet: On the one hand,the active camera system should be able to identify an outstretched index finger and zoom in on its tip in order to better display what is being shown. On the other hand, the zoom should be controlled by a gesture using a flat palm with an extended thumb, so that the entire control of the software, after an initial start, can be done hands-free.\n",
        "\n",
        "In this documentation, we would like to show how we implemented our approach to an interactive camera and explain each aspect in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWXI_QNdxeE7"
      },
      "source": [
        "#Finger recognition\r\n",
        "As described above, one of the goal of the interactive camera is to track an outstretched finger based on the fingertip. Therefore, the first problem we encounter, would be to have the interactive camera recognize a hand overall and consequently isolate an outstretched finger. The following figure shows a processing pipeline of all steps, which will now be discussed in more detail.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_eC-4nVxhVF"
      },
      "source": [
        "## Histogram creation\r\n",
        "To first detect a hand, a histogram-assisted masking of the given camera image is performed. A histogram represents a frequency distribution of individual pixels in an image. If a color is represented by a high number of pixels, this color has a high frequency or intensity in the histogram. Therefore, histograms are useful for filtering differently colored objects in a scene so that these filtered objects can be used in binary representation for more convenient use. The problems, which arise from hands of a different skin color than the developer's, can hence be avioded as there would be no neglection of any skin type anymore. However, this requires histogram creation by the user before the actual software is being used. Thus, as in Figure XX, the user needs to place his hand under the auxiliary areas drawn in and create the histogram by pressing the \"Z\" key.\r\n",
        "\r\n",
        "![alt Text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/18.11.20/Messung_Hautfarbe.png \"Bild 7: Messpunkte auf Hand\")\r\n",
        "\r\n",
        "**Figure XX:** Measuring points on hand \r\n",
        "\r\n",
        "This is done by merging the drawn areas into an independent image and then using \"[*OpenCV*](https://pypi.org/project/opencv-python/)\" to create the required histogram. The following code example shows how the measurement areas are drawn onto a given image and how a histogram is created from them. It should be mentioned that the calculated histogram is normalized to reduce any noise.\r\n",
        "```python\r\n",
        "  def drawMeasuringRectangles(frame):\r\n",
        "    \"\"\"Draws 'amountOfMeasuringRectangles' Rectangles on the given frame and returns the modified image\"\"\"\r\n",
        "    rows, cols, dontCare = frame.shape\r\n",
        "    global amountOfMeasuringRectangles, xCoordinatesOfMeasuringRectangles_topLeft, yCoordinatesOfMeasuringRectangles_topLeft, xCoordinatesOfMeasuringRectangles_bottomRight, yCoordinatesOfMeasuringRectangles_bottomRight\r\n",
        "\r\n",
        "    # position messure points of hand histogram\r\n",
        "    xCoordinatesOfMeasuringRectangles_topLeft = np.array(\r\n",
        "        [6 * rows / 20, 6 * rows / 20, 6 * rows / 20, 9 * rows / 20, 9 * rows / 20, 9 * rows / 20, 12 * rows / 20,\r\n",
        "         12 * rows / 20, 12 * rows / 20], dtype=np.uint32)\r\n",
        "\r\n",
        "    yCoordinatesOfMeasuringRectangles_topLeft = np.array(\r\n",
        "        [9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20,\r\n",
        "         10 * cols / 20, 11 * cols / 20], dtype=np.uint32)\r\n",
        "\r\n",
        "    # define shape of drawn small rectangles | here 10x10\r\n",
        "    xCoordinatesOfMeasuringRectangles_bottomRight = xCoordinatesOfMeasuringRectangles_topLeft + 10\r\n",
        "    yCoordinatesOfMeasuringRectangles_bottomRight = yCoordinatesOfMeasuringRectangles_topLeft + 10\r\n",
        "\r\n",
        "    # draw calculated rectangles\r\n",
        "    for i in range(amountOfMeasuringRectangles):\r\n",
        "        cv2.rectangle(frame,\r\n",
        "                      (yCoordinatesOfMeasuringRectangles_topLeft[i], xCoordinatesOfMeasuringRectangles_topLeft[i]),\r\n",
        "                      (yCoordinatesOfMeasuringRectangles_bottomRight[i],\r\n",
        "                       xCoordinatesOfMeasuringRectangles_bottomRight[i]),\r\n",
        "                      (0, 255, 0), 1)\r\n",
        "\r\n",
        "    return frame\r\n",
        "\r\n",
        "  def createHandHistogram(frame):\r\n",
        "    global xCoordinatesOfMeasuringRectangles_topLeft, yCoordinatesOfMeasuringRectangles_topLeft\r\n",
        "\r\n",
        "    # convert cv2 bgr colorspace to hsv colorspace for easier handling\r\n",
        "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\r\n",
        "    # create new blank Region Of Interest matrix/image\r\n",
        "    roi = np.zeros([90, 10, 3], dtype=hsv_frame.dtype)\r\n",
        "\r\n",
        "    # fill ROI with the sample rectangles\r\n",
        "    for i in range(amountOfMeasuringRectangles):\r\n",
        "        roi[i * 10: i * 10 + 10, 0: 10] = hsv_frame[xCoordinatesOfMeasuringRectangles_topLeft[i]:\r\n",
        "                                                    xCoordinatesOfMeasuringRectangles_topLeft[i] + 10,\r\n",
        "                                          yCoordinatesOfMeasuringRectangles_topLeft[i]:\r\n",
        "                                          yCoordinatesOfMeasuringRectangles_topLeft[i] + 10]\r\n",
        "\r\n",
        "    # create a Hand histogram and normalize it\r\n",
        "    hand_hist = cv2.calcHist([roi], [0, 1], None, [180, 256], [0, 180, 0, 256])\r\n",
        "\r\n",
        "    # remove noise and retun\r\n",
        "    return cv2.normalize(hand_hist, hand_hist, 0, 255, cv2.NORM_MINMAX)\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTSQtxxpxnRu"
      },
      "source": [
        "##Masking the hand\r\n",
        "Once a histogram is created, it can be used to subdivide a given image into gray levels using a so-called *backProjection* by OpenCV. A white pixel indicates that this place in the original image represents the color of the most intense color in the histogram, while a black pixel indicates the opposite. Thus,it is already possible to recognize the desired object in the resulting image. \r\n",
        "\r\n",
        "However, as can be seen in Fig. XY, this is still quite noisy and does not yet show the aforementioned binary image. \r\n",
        "\r\n",
        "![altText](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/18.11.20/Hand_Nach_Histogram_BackProjection.png)\r\n",
        "\r\n",
        "**Bild XY:** Hand after `cv2.calcBackProject( [...] )`\r\n",
        "\r\n",
        "```python\r\n",
        " hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\r\n",
        "\r\n",
        "    # mask area that matches with the histogram via back projection\r\n",
        "    histogramMaskBackProjection = cv2.calcBackProject([hsv], [0, 1], hist, [0, 180, 0, 256], 5)\r\n",
        "```\r\n",
        "\r\n",
        "Furthermore, isolated false positives can be detected, which appear at the edge of the image around the searched object. To solve these problems, a closing and subsequent opening operation is required, which in turn is followed by a threshold holding operation. In figure YY, one can see that the to be recognized hand already has fewer gaps. Yet false positives can still be detected. That is the case, a closing operation closes smaller holes, i.e. black pixels, and thus allows white pixels that are close to each other to grow.\r\n",
        "\r\n",
        "![altText](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/Hand_nach_closing_operation.png)\r\n",
        "\r\n",
        "**Figure YY:** Hand nach *Closing-Operation*\r\n",
        "\r\n",
        "```python\r\n",
        "maskingCircle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\r\n",
        "\r\n",
        "closedBackProjection = cv2.morphologyEx(histogramMaskBackProjection, cv2.MORPH_CLOSE,maskingCircle, iterations=2)\r\n",
        "```\r\n",
        "\r\n",
        "The resulting undesired effect of larger false positive values can be counteracted by an opening operation. An opening operation removes smaller objects, i.e. white pixels, and thus removes almost any false positive values as can be seen in Figure YZ.\r\n",
        "\r\n",
        "![altText](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/Hand_nach_opening_operation.png)\r\n",
        "\r\n",
        "**Figure YZ:** Hand after *Opening-Operation*\r\n",
        "\r\n",
        "```python\r\n",
        "openedBackProjection = cv2.morphologyEx(closedBackProjection, cv2.MORPH_OPEN,maskingCircle, iterations=2)\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "Finally, the image must be converted to a binary, i.e. black and white, image for easier handling later on. That can be done by a simple thresholding operation, which turns every pixel that is not black white.\r\n",
        "\r\n",
        "A result of this operation can be seen in Figure ZZ.\r\n",
        "\r\n",
        "![altText](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/Hand_nach_Threshholding.png)\r\n",
        "\r\n",
        "**Figure ZZ:** Hand nach *Threshholding-Operation*\r\n",
        "\r\n",
        "```python\r\n",
        "ret, thresh = cv2.threshold(openedBackProjection, 1, 255, cv2.THRESH_BINARY)\r\n",
        "\r\n",
        "thresh = cv2.merge((thresh, thresh, thresh))\r\n",
        "\r\n",
        "cv2.bitwise_and(frame, thresh)\r\n",
        "```\r\n",
        "\r\n",
        "As shown in the code example, after the thresholding operation, a new image is generated using the ``cv2.merge`` command, which uses the color values of the thresholding operation in each of the RGB channels. This produces a result as shown in Figure ZZ. This binary image is then used to mask the original image appropriately by the ``cv2.bitwise_and`` command, so that only the relevant area of the hand is colored. The rest of the image remains black.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAqCQ_9yx0VS"
      },
      "source": [
        "#Unsuccessful approach with Machine Learning\r\n",
        "Before the histogram-based hand recognition was implemented, there was an attempt to recognize a pointing hand in an image using machine learning. However, the acquisition of data proved to be a challenge. Different lighting conditions can have a strong negative impact on the results, so data must be collected under different lighting conditions. That is tedious and difficult to automate. **Therefore, the training data amounted to only a few hundred images, which is why a correspondingly sobering result can be heard here, as far as the hit rate of the model is concerned.** That means that pointing and non-pointing hands can only be distinguished with difficulty. In addition, actions such as for example as writing or wiping, ensure that some false positives are detected. Thus,machine learning has proven to be insufficiently accurate at this point, which is why this approach was discarded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OrKNYvrxpyj"
      },
      "source": [
        "##Finger tip recognition\r\n",
        "To recognize the user's fingertip, the software uses a contour-based solution. OpenCV is able to recognize the contour of a shown object by means of an image as the following code shows. \r\n",
        "```python\r\n",
        "def getContoursFromMaskedImage(maskedHistogramImage):\r\n",
        "    \"\"\"Returns the contours of a given masked Image\"\"\"\r\n",
        "    grayscaledMaskedHistogramImage = cv2.cvtColor(maskedHistogramImage, cv2.COLOR_BGR2GRAY)\r\n",
        "    ret, thresh = cv2.threshold(grayscaledMaskedHistogramImage, 0, 255, 0)\r\n",
        "    cont, hierarchyDontCare = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\r\n",
        "    return cont\r\n",
        "```\r\n",
        "Using the list of straight lines obtained, i.e. the contours, the center of the contour is determined in order to obtain the center of the hand as well. This is important later to draw a line to an outstretched finger. The following code shows how the center of mass of a series of points can be found using so-called *moments*. The points are represented here by the ends of the vectors describing the contour.\r\n",
        "```python\r\n",
        "def getCenterCoordinatesOfContour(maxContour):\r\n",
        "    \"\"\"Returns the Centercoordinates of a given contour in the shape  X, Y\"\"\"\r\n",
        "    moment = cv2.moments(maxContour)\r\n",
        "    if moment['m00'] == 0:\r\n",
        "        return None\r\n",
        "    cx = int(moment['m10'] / moment['m00'])\r\n",
        "    cy = int(moment['m01'] / moment['m00'])\r\n",
        "    return cx, cy\r\n",
        "```\r\n",
        "In addition, it is possible to form a convex (i.e. outwardly curved) envelope through the contour, which in turn can be used to detect convex defects. A convex defect is characterized by the fact that a point is not located on the line of the envelope, but inside it. Figure ZA shows this behavior using the gray hand line and the red convex hull. \r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/Zeichnung_konvexeHuelle_mit_Defekten.png\" width=50%>\r\n",
        "<img src=\"https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/Zeichnung_maximaler_konvexer_defekt.png\" width = 40%>\r\n",
        "\r\n",
        "**Figure ZA:** Convex hull with general (left) and maximum (right) convex defects drawn in.\r\n",
        "\r\n",
        "With the help of all convex defects and the center of mass, the most distant point of the shell can be identified, which is assumed to be the fingertip. Thus, the point or the defect is searched for, which has the biggest distance to the center.\r\n",
        "\r\n",
        "These two points are recalculated for each frame and added to a list of points in order to determine the average center of both point lists over time. It ensures that the points of the center of the hand and those of the recognized fingertip are less volatile. I.e. on the basis of an average point over time possible measuring errors have a small deflection and fall so for further computations less into weight. However, it should be noted here that a long list must be generated over a longer period of time or more frames, which means that the average center point can only move sluggishly. This may be desirable, as it allows the point to move smoothly and not make \"jumps\". However, this can result in the desired center point moving too slowly and thus becoming unusable. Therefore, finding an optimal number of points is of high importance. By trial and error, a value of 25 points has turned out to be the optimal number.\r\n",
        "Figure AA shows here in yellow the points of the fingertip with their average center in green. In addition, the center of the hand and its average center can be seen here in pink and red, respectively.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/Hauptkamera_Mit_Infos.png\" width = 100%>\r\n",
        "\r\n",
        "**Figure AA:** Detected points of the fingertip and the center of the hand with their average centers\r\n",
        "\r\n",
        "The large unfilled circles in red and green represent another control instance here. Points that appear outside these radii are discarded during processing in order to ignore possible misrecognized points. This means that in this frame the respective point is not recognized and therefore this frame does not contribute to the creation of the point list. This kind of self-checking is based on the assumption that the user does not move his hand frantically and thus could fall outside the radius. If this is the case, the system paralyzes itself and all points remain at their last position. This undesired behavior can easily be corrected by the user moving his hand again to the last detected position and resuming the tracking from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJpJwL7cxrgV"
      },
      "source": [
        "#Zoom to the fingertip\r\n",
        "Since what is shown may be too small for the image in the image section, a digital zoom is required. It should focus on the user's fingertip. However, the focus should not be directly on the user's fingertip at first, but just above it, since the focus would otherwise not be on what is shown, but on the finger itself. This is not desirable. Therefore, a line or vector is drawn from the center of the hand to the fingertip, which is then extended by 50%. The end of the vector thus displays the center of the image to be focused.\r\n",
        "A previously defined but variable zoom factor determines the size of the newly calculated image section. The image section is determined in a way that, starting from the center of the image, the respective corner coordinates are determined with the following formula: \r\n",
        "```python\r\n",
        "#frame.shape[1] := width ; frame.shape[0] := height\r\n",
        "leftX, rightX = int(xCenterOfNewFrame - frame.shape[1] // zoomFactor // 2), int(xCenterOfNewFrame + frame.shape[\r\n",
        "        1] // zoomFactor // 2)\r\n",
        "    bottomY, topY = int(yCenterOfNewFrame - frame.shape[0] // zoomFactor // 2), int(yCenterOfNewFrame + frame.shape[\r\n",
        "        0] // zoomFactor // 2)\r\n",
        "```\r\n",
        "After this calculation, it is important to check if the new coordinates are actually inside the original image. If that is not the case, as in Figure AB, the outer coordinates must be moved to the nearest edge so that the new image section is at the edge of the original image.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/Verschobener_Frame.png\" width = 100%>\r\n",
        "\r\n",
        "**Figure AB:** Illegal image cropping of the zoomed window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOsgx-5ShwKg"
      },
      "source": [
        "# Gesture recognition with Tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHLAoTMKiCni"
      },
      "source": [
        "## Tensorflow as our Machine-Learning Software\n",
        "Machine learning is an important part of our gesture recognition. The software available to us today is far more powerful than we needed in this case. This is also the case with TensorFlow. Nevertheless, we decided to use this software because it is easy to implement and no further knowledge is required for the initial setup. In addition, we are already provided with more in-depth information on the application of this in the course of the event."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V6q6g7xiE8v"
      },
      "source": [
        "## Aquire training data\n",
        "In order to achieve a high degree of consistency in the recognized gestures, a large data set is needed. \n",
        "It also has to considered in which part of the software the gesture recognition should take place. First of all, there are many options. It would be possible to recognize the gesture before any processing of the image. However, this would cause a lot of problems. For example, hundreds of pictures of each gesture with different lighting conditions, skin colors and backgrounds would have to be taken in order to achieve even a rudimentarily accurate result. Another possibility for gesture recognition would come after backprojection. What is most important for gesture recognition has already been filtered out: the hand. At the same time it results in an image that is only available in black and white and would not need the background, nor the skin color for training. However, there are still some artifacts to be seen, as certain areas of the image have a similar hue, but do not belong to the hand. Therefore, the best step would be the last step of the processing: Thresholding. As already described above, the occurring artifacts are filtered during backprojection. \n",
        "The important areas are additionally highlighted as well.\n",
        "\n",
        "Now, to get as much data as possible, over 1000 images per gesture need to be collected. The data were created rather quickly due minimal focus on lighting conditions and the lack of attention on background or skin color. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YGLEtzMieD1"
      },
      "source": [
        "### Live video capture of gestures we want to recognise\n",
        "To collect the data, several options were possible. On the one hand, the software itself could store the processed images. One would only have to sift through them once and sort out any inaccurate results. However, this would have a strong impact on the performance and slow down the creation of the data set. It was deemed more useful to record the displayed output of the processed frame as a screen video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nogytwtQi5zg"
      },
      "source": [
        "## Prepare training data\n",
        "Since the created video cannot simply serve as training data in TensorFlow, they had to be further prepared beforehand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yClu5aEei9M9"
      },
      "source": [
        "### Converting the Videos to Images, cropping and resizing\n",
        "The created videos were converted into a sequence of images. In addition,the images were reduced to the relevant area for us. These images also had to be sifted afterwards in order to sort out errors. Since a tensor flow model works with an input of 224 x 224 images, the images were scaled to the same size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj7iGjofjC5v"
      },
      "source": [
        "## Training of the Model\n",
        "The training of the model could now be started. \n",
        "There is a very helpful website (https://teachablemachine.withgoogle.com), which handles the entire training of the model and provides suitable training methods depending on the different domains of usage. \n",
        "The Image Classification model was chosen for this purpose.\n",
        "![TeachableMachine.com](https://github.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/blob/main/Documentation/Pictures/27.01.21/teachableMachine.jpg?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL-HKuRPjnBi"
      },
      "source": [
        "## Implementation of the Model\n",
        "The actual use of the code turned out to be rather simple. After the model was trained, a opportunity provided by the website arised to take a simple example of the implementation of the Keras model in Python directly. An adaption had to take place for their template to fit into the approach chosen.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQEb7jFokWaJ"
      },
      "source": [
        "def getGesturePredictionFromTensorflow(frame, model):\n",
        "    if frame is None or model is None or type(frame) != np.ndarray or type(model) != tf.keras.Sequential:\n",
        "        return \"OTHER\"\n",
        "    h1 = frame.shape[0]\n",
        "    w1 = frame.shape[1]\n",
        "\n",
        "    # Create the array of the right shape to feed into the keras model\n",
        "    # The 'length' or number of images you can put into the array is\n",
        "    # determined by the first position in the shape tuple, in this case 1.\n",
        "    data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
        "\n",
        "    # Replace this with the path to your image\n",
        "    dimension = (224, 224)\n",
        "    image = cv2.resize(frame, dimension, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # turn the image into a numpy array\n",
        "    image_array = np.asarray(image)\n",
        "\n",
        "    # Normalize the image\n",
        "    normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
        "\n",
        "    # Load the image into the array\n",
        "    data[0] = normalized_image_array\n",
        "\n",
        "    # run the inference\n",
        "    prediction = model.predict(data)\n",
        "\n",
        "    # print(prediction)\n",
        "    predictionDictionary = {\n",
        "        \"LEFT\": prediction[0][0],\n",
        "        \"RIGHT\": prediction[0][1],\n",
        "        \"OTHER\": prediction[0][2]\n",
        "    }\n",
        "    global lastDetection, lastDetectionCount\n",
        "    detection = max(predictionDictionary.items(), key=operator.itemgetter(1))[0]\n",
        "    if lastDetection is None or lastDetection != detection:\n",
        "        lastDetection = detection\n",
        "        lastDetectionCount = 0\n",
        "    else:\n",
        "        lastDetectionCount += 1\n",
        "\n",
        "    return detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz3mc2YTjpPe"
      },
      "source": [
        "# GUI\n",
        "The software presented here, which is to be used as a collaborative tool, needs a GUI just because of a live preview. \n",
        "Therefore, one of the aims was to make it as simple and clear as possible. It also had to be considered that the software will be used with systems that have multiple cameras and screens. Therefore, a way to switch between the different monitors and cameras as easily as possible had to be created. This also without restarting the software.\n",
        "It was also important to think about how to display multiple windows that reflect different steps in the processing of the image and thus visualize our processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHDf-Bq90CIn"
      },
      "source": [
        "class ImageShower(object):\n",
        "    \"\"\"Creates another TKInter Window and shows the given Image\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name=\"Window\", window=None):\n",
        "        \"\"\"\n",
        "        Initialize a new ImageShower, by creating another TKInter Window and set its Name\n",
        "        :param name:\n",
        "        \"\"\"\n",
        "        if window is None:\n",
        "            self.window = tk.Toplevel(app)\n",
        "            self.window.title(name)\n",
        "        else:\n",
        "            self.window = window\n",
        "\n",
        "        self.panel = None\n",
        "        self.frame = None\n",
        "\n",
        "    def update(self, image):\n",
        "        \"\"\"\n",
        "        Update the Image witch will be shown in this Window\n",
        "        :param image: The Image as cv2 Image in BGR\n",
        "        \"\"\"\n",
        "        self.frame = image\n",
        "\n",
        "    def show(self, width=640, height=360):\n",
        "        \"\"\"\n",
        "        Shows the Image, witch has been already set by the Update Method or is given by an Optional Parameter\n",
        "        :param frame: The Optional cv2 Image in BGR\n",
        "        :param width: The Optional scaled Width of the Image\n",
        "        :param height: The Optional scaled Height of the Image\n",
        "        :return: None if no Image is given\n",
        "        \"\"\"\n",
        "        if self.frame is None:\n",
        "            return\n",
        "        try:\n",
        "            # Resize and Convert cv2 Image to TKInter Image\n",
        "            img = cv2.resize(np.array(self.frame), (width, height), interpolation=cv2.INTER_AREA)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\n",
        "            img = Image.fromarray(img)\n",
        "            img = ImageTk.PhotoImage(img)\n",
        "            # if the panel is not None, we need to initialize it\n",
        "            if self.panel is None:\n",
        "                self.panel = tk.Label(self.window, image=img)\n",
        "                self.panel.image = img\n",
        "                self.panel.pack(side=tk.TOP)\n",
        "\n",
        "            # otherwise, simply update the panel\n",
        "            else:\n",
        "                self.panel.configure(image=img)\n",
        "                self.panel.image = img\n",
        "        except RuntimeError:\n",
        "            print(\"[INFO] caught a RuntimeError\")\n",
        "        except cv2.error:\n",
        "            print(\"[DEBUG] Bildfehler! (Format richtig?)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5CpfVtyjtf-"
      },
      "source": [
        "## Showing Windows\n",
        "The first step was to display the current monitor within the software. For this purpose the Python library 'mss' was used. It is able to read all connected monitors and to display data like the current screen content or the dimensions of the selected monitor. \n",
        "\n",
        "Some of the progress steps required precise adjustments to the parameters. For this purpose, some steps needed to be displayed with additional information. Subsequently ,relevant information can be extrated as to how the software reacts in certain situations.\n",
        "\n",
        "It would be a possibility to use the ImageShower shown earlier in order to achieve that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmI3_82m0zN_"
      },
      "source": [
        "# Create Optional Windows for Debugging and Additional Infos\n",
        "histogramWindow = ImageShower(\"Histogram\")\n",
        "histogramThreshWindow = ImageShower(\"Histogram mit Threshhold\")\n",
        "mainCameraWithInfo = ImageShower(\"Hauptkamera mit Infos\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXG7TCukjvuw"
      },
      "source": [
        "### Live Camerafeed with generated metadata\n",
        "In order to see how the software performs on different devices, the current frame rate is displayed on the processed frame.\n",
        "\n",
        "This contains further data such as an activation circle, the last recognized positions of the finger, as well as the assumed position of the back of the hand. Also available to see in the view, is the current zoom level.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fZUzUw9jyZf"
      },
      "source": [
        "### Processed Image with Backprojection\n",
        "Another output represents a specific point in the actual image processing. After a histogram has been recorded, it is applied to the current camera image using backprojection. The result is all pixels that match parts of the histogram. All the other parts of the image are black. This display was valuable to use because it provided important information about the processing steps that had already been performed. Moreover, it showed whether various changes in the size of the histogram or in the parameters of the backprojection produced more positive results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVsOubqzj0Q6"
      },
      "source": [
        "### Processed Image with additional Thresholding\n",
        "Another processing step that was used for debugging purposes was a small window showing the processed camera image after the additional thresholding. Various previously performed processing steps played a major role in the final quality. An example would be different lighting conditions, or different skin tones on the back and palm of the hand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOB8z0rtj3Ef"
      },
      "source": [
        "### Main-Window (Screen + PiP)\n",
        "To bring all the processing steps together, there is a main window. It contains both the choice between different monitors and cameras, as well as the display of the selected monitor and the processed picture of the camera. The camera image is then only displayed when a finger is in the image. In addition, the image can be zoomed in or out using the aforementioned gesture recognition. The zoomed image always follows the finger and zooms to the displayed position. The zoom level is maintained even if the finger leaves the picture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNJm6kDAnA_5"
      },
      "source": [
        "## Performance Improvements\n",
        "An issue that was detected quite quickly was the performance drop after not only the current monitor was displayed in the window, but also the incoming camera image was processed. The problem with the software was that all actions happened on one thread: both the reading of the monitor, the camera, the entire processing and the subsequent display of the results. \n",
        "The solution was discussed that some sections of the program could be outsourced to separate threads in order to already read in the image that was to be processed and make it available by means of a variable.\n",
        "To resolve the issue, two different program sections were programmed, which separately take care of the camera to be read in as well as the reading of the monitor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbINdPXOzC2J"
      },
      "source": [
        "class MonitorGrabber(object):\n",
        "    \"\"\"\n",
        "    Reads the Current Screen in another Thread and Stores it for easy Access\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, src=1, width=1280, height=720):\n",
        "        \"\"\"\n",
        "        Initialize a new MonitorGrabber\n",
        "        :param src: MonitorIndex from mss\n",
        "        :param width: Scaled Output Image width\n",
        "        :param height: Scaled Output Image hight\n",
        "        \"\"\"\n",
        "        self.setSrc(src)\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "        # Grab Monitor Image, Resize, Convert and Store it\n",
        "        img = sct.grab(self.src)\n",
        "        # noinspection PyTypeChecker\n",
        "        img = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        self.picture = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "        self.stopped = False\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"\n",
        "        Starts another Thread for its own get-Method, to grab the Image out of Mainloop\n",
        "        :return:  Optional: The Own Object to create, start the Thread and save the Object at the same Time\n",
        "        \"\"\"\n",
        "        Thread(target=self.get, args=()).start()\n",
        "        return self\n",
        "\n",
        "    def setSrc(self, src):\n",
        "        \"\"\"\n",
        "        Re-Sets the Monitor Input Source Index of mss\n",
        "        :param src: The new Monitor Index\n",
        "        \"\"\"\n",
        "        self.src = sct.monitors[src]\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Grabs the current Monitor Image, Resize, convert and stores it\n",
        "        \"\"\"\n",
        "        while not self.stopped:\n",
        "            img = sct.grab(self.src)\n",
        "            img = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "            self.picture = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"\n",
        "        Stops the MonitorGrabber-Get-Thread started by the start-Method\n",
        "        \"\"\"\n",
        "        self.stopped = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZsRj2Y_zHKy"
      },
      "source": [
        "class CameraGrabber(object):\n",
        "    \"\"\"\n",
        "    Reads the Current Camera-feed in another Thread and Stores it for easy Access\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, src, width=1280, height=720):\n",
        "        \"\"\"\n",
        "        Initialize a new CameraGrabber\n",
        "        :param src: CameraIndex from mss\n",
        "        :param width: Scaled Output Image width\n",
        "        :param height: Scaled Output Image hight\n",
        "        \"\"\"\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "        # Grab Camera Image, Resize, Convert and Store it\n",
        "        self.stream = cv2.VideoCapture(src)\n",
        "        (self.grabbed, img) = self.stream.read()\n",
        "        self.picture = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        self.stopped = False\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"\n",
        "        Starts another Thread for its own get-Method, to grab the Image out of Mainloop\n",
        "        :return:  Optional: The Own Object to create, start the Thread and save the Object at the same Time\n",
        "        \"\"\"\n",
        "        Thread(target=self.get, args=()).start()\n",
        "        return self\n",
        "\n",
        "    def setSrc(self, src):\n",
        "        \"\"\"\n",
        "        Re-Sets the Camera Input Source Index of mss\n",
        "        :param src: The new Camera Index\n",
        "        \"\"\"\n",
        "        self.stream = cv2.VideoCapture(src)\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Grabs the current Camera Image, Resize and stores it\n",
        "        \"\"\"\n",
        "        while not self.stopped:\n",
        "            if not self.grabbed:\n",
        "                self.stop()\n",
        "            else:\n",
        "                (self.grabbed, img) = self.stream.read()\n",
        "                self.picture = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"\n",
        "        Stops the CameraGrabber-Get-Thread started by the start-Method\n",
        "        \"\"\"\n",
        "        self.stopped = True"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}