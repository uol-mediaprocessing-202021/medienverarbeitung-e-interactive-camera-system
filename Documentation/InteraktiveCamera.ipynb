{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "InteraktiveCamera.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/blob/main/Documentation/InteraktiveCamera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl34ucqPSTUC"
      },
      "source": [
        "# **Grundlegende Idee**\n",
        "Die Idee ist ein Zwei-Kamera-System zu einem Ein-Kamera-System umzugestalten. Dabei soll die erste Kamera auf einen Sprecher und die zweite Kamera auf ein Blatt Papier gerichtet sein. Sobald der Sprecher nun seinen Zeigefinger auf das Papier legt, wird dieses Kamerabild als Bild in Bild (pip) der ersten Kamerasicht angezeigt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASWMfoeLbGr1"
      },
      "source": [
        "## Benötigte Hardware\n",
        "Das System soll so einfach wie möglich sein und keine spezielle Hardware benötigen. Das heißt eine Handykamera, welche über Apps wie beispielsweise [Droid Cam](http://www.dev47apps.com/) auf Android einen Kamerafeed an den PC liefern sollen schon ausreichen. Dementsprechend soll es auch möglich sein dieses Handykamerabild innerhalb einer Bildschirmübertragung anzuzeigen, um so gezeigtes auf dem Bildschirm durch beispielsweise Handzeichnungen zu unterstützen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MMYSMlbJMx"
      },
      "source": [
        "## Funktionen\n",
        "Wie bereits genannt wird der Nutzer zwei Bildinputs auswählen und festlegen müssen welche der Inputs das einzublendende Bild enthält. Dem Nutzer soll es möglich sein durch Zeigen mit dem Zeigefinger die Kamera zu Aktivieren und den Zeigefinger als Mittelpunkt des Bildes zu sehen. Durch eine \"Raus-Zoom\" Geste wie man sie von Smartphones kennt, wird ein digitaler Zoom ausführbar sein."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VvNiAGPbD6u"
      },
      "source": [
        "## Programmcode\n",
        "Jeglicher Programmcode ist in dem folgende Repository zu finden:\n",
        "[Medienverarbeitung Gruppe E - Interactive Camera](https://github.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2TPDKR8bXAx"
      },
      "source": [
        "## Vorgehen im Projekt\n",
        "Das Projekt wird als Projekttagebuch dokumentiert. Hier werden alle Ideen, Ansätze und Ergebnisse festgehalten. Zu Abschluss des Projekts wird das Gesamtergebnis noch einmal gesondert und aufbereitet präsentiert. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thBxNuIcqqI"
      },
      "source": [
        "# Projekttagebuch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxZ5eNO1c03y"
      },
      "source": [
        "## Erster Ansatz und Ergebnisse \n",
        "*11.11.20*\n",
        "\n",
        "Zu erst kümmern wir uns um die Erkennung eines zeigenden Zeigefingers. Dafür trainieren wir ein Keras Modell mit Bildern, welche zeigende Zeigefinger (Bild 1), weiße (karrierte, linierte, blanko) Papiere (Bild 2) und andere Gesten wie flache Hände und Ähnliches (Bild 3) unterscheiden können soll.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/11.11.20/zu_unterscheidende_Objekte.png)\n",
        "\n",
        "Das damit antrainierte Modell erkennt das gezeigte Bild und sortiert es der Rubrik ein, welche mit der höchsten Konfidenz übereinstimmt.\n",
        "\n",
        "Die Verarbeitung des eingehenden Videostreams wird durch die Python-Bibliothek \"[*OpenCV*](https://pypi.org/project/opencv-python/)\" ermöglicht.\n",
        "\n",
        "Durch diesen Ansatz konnten bisher erste Erfolge und enstehende Probleme verzeichnet werden\n",
        "\n",
        "###Probleme\n",
        "Es bisher Störfaktoren wie eine Computer-Maus (Bild 6) fälschlicherweise als ein Zeigen erkannt. Jedoch war es auch möglich in einer kontrollierten Umgebung einen Zeigefinger (Bild 4) von einem Blatt Papier (Bild 5) zu unterscheiden.\n",
        "![alt text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/11.11.20/Ergebnisse.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVF_fn3fwpQZ"
      },
      "source": [
        "##Verwurf vorheriger Ergebnisse und entdeckung neuer Technologien\n",
        "*18.11.2020*\n",
        "\n",
        "Eine Objekterkennung durch ein neurales Netzwerk bedarf einer ausgesprochen Großen Datenmenge, welche kuriert wurden, um sie auf Relevante objekte zu reduzieren. Dies heißt in unserem Fall, dass wir einen für zwei Personen nicht erzeugbaren Datensatz an zeigenden Fingern und Hintergrundbildern benötigen würden, um akkurate Ergebnise zu erziehlen. Daher brauchen wir eine Alternative, welche mit einem realistischen Arbeitsaufwand zu bewältigen ist.\n",
        "\n",
        "Nun stellt sich die Frage: Was macht einen zeigenden Finger aus? Oder noch simpler: Was macht eine Hand aus?\n",
        "\n",
        "Wir als Menschen erkennen Objekte auf Distanzen primär über Form und Farbe. So entsteht die Idee, dass ein Computer ein Bild nach der Handfarbe eines Nutzers absuchen und diesen Bereich isolieren kann. Dies ist in \"[*OpenCV*](https://pypi.org/project/opencv-python/)\" über sogenannte Histogramme möglich. \n",
        "Die Grundlegende Idee ist die solche, dass anfangs die Hautfarbe des Nutzers durch eine simple Bildprobe ermittelt wird und so eine Hand (bzw. etwas Hautfarbenes) in einem Bild erkannt werden kann. Dabei besteht die Annahme, dass auch nur *eine* Hand im Bild zu sehen ist und keine weiteren mit der Hautfarbe übereinstimmenden Gegenstände im Bild sind.\n",
        "\n",
        "![alt Text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/18.11.20/Messung_Hautfarbe.png \"Bild 7: Messpunkte auf Hand\")\n",
        "\n",
        "**Bild 7:** Messpunkte auf Hand\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In Bild 7 ist eine solche Messung zu sehen. Die dabei markierten Rechtecke werden zu einem Bild zusammengefügt und aus diesem entstehenden Bild ein Histogram errechnet. Hier zu sehen ist, wie diese Regionen markiert und für die Messung genutzt werden:\n",
        "\n",
        "\n",
        "```python\n",
        "def drawMeasuringRectangles(frame):\n",
        "    \"\"\"Draws 'amountOfMeasuringRectangles' Rectangles on the given frame and returns the modified image\"\"\"\n",
        "    rows, cols, dontCare = frame.shape\n",
        "    global amountOfMeasuringRectangles, xCoordinatesOfMeasuringRectangles_topLeft, yCoordinatesOfMeasuringRectangles_topLeft, xCoordinatesOfMeasuringRectangles_bottomRight, yCoordinatesOfMeasuringRectangles_bottomRight\n",
        "\n",
        "    # position messure points of hand histogram\n",
        "    xCoordinatesOfMeasuringRectangles_topLeft = np.array(\n",
        "        [6 * rows / 20, 6 * rows / 20, 6 * rows / 20, 9 * rows / 20, 9 * rows / 20, 9 * rows / 20, 12 * rows / 20,\n",
        "         12 * rows / 20, 12 * rows / 20], dtype=np.uint32)\n",
        "\n",
        "    yCoordinatesOfMeasuringRectangles_topLeft = np.array(\n",
        "        [9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20,\n",
        "         10 * cols / 20, 11 * cols / 20], dtype=np.uint32)\n",
        "\n",
        "    # define shape of drawn small rectangles | here 10x10\n",
        "    xCoordinatesOfMeasuringRectangles_bottomRight = xCoordinatesOfMeasuringRectangles_topLeft + 10\n",
        "    yCoordinatesOfMeasuringRectangles_bottomRight = yCoordinatesOfMeasuringRectangles_topLeft + 10\n",
        "\n",
        "    # draw calculated rectangles\n",
        "    for i in range(amountOfMeasuringRectangles):\n",
        "        cv2.rectangle(frame,\n",
        "                      (yCoordinatesOfMeasuringRectangles_topLeft[i], xCoordinatesOfMeasuringRectangles_topLeft[i]),\n",
        "                      (yCoordinatesOfMeasuringRectangles_bottomRight[i],\n",
        "                       xCoordinatesOfMeasuringRectangles_bottomRight[i]),\n",
        "                      (0, 255, 0), 1)\n",
        "\n",
        "    return frame\n",
        "\n",
        "  def createHandHistogram(frame):\n",
        "    global xCoordinatesOfMeasuringRectangles_topLeft, yCoordinatesOfMeasuringRectangles_topLeft\n",
        "\n",
        "    # convert cv2 bgr colorspace to hsv colorspace for easier handling\n",
        "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    # create new blank Region Of Interest matrix/image\n",
        "    roi = np.zeros([90, 10, 3], dtype=hsv_frame.dtype)\n",
        "\n",
        "    # fill ROI with the sample rectangles\n",
        "    for i in range(amountOfMeasuringRectangles):\n",
        "        roi[i * 10: i * 10 + 10, 0: 10] = hsv_frame[xCoordinatesOfMeasuringRectangles_topLeft[i]:\n",
        "                                                    xCoordinatesOfMeasuringRectangles_topLeft[i] + 10,\n",
        "                                          yCoordinatesOfMeasuringRectangles_topLeft[i]:\n",
        "                                          yCoordinatesOfMeasuringRectangles_topLeft[i] + 10]\n",
        "\n",
        "    # create a Hand histogram and normalize it\n",
        "    hand_hist = cv2.calcHist([roi], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
        "\n",
        "    # remove noise and retun\n",
        "    return cv2.normalize(hand_hist, hand_hist, 0, 255, cv2.NORM_MINMAX)\n",
        "```\n",
        "Das daraus entstehende Histogram wird verwendet, um auf das gegebene Bild bzw. den Videoframe einen Graustufenfilter anzuwenden. Dabei stimmen hellere (weißere) Pixel mehr mit dem errechneten Histogram überein, während dunklere (schwarze) Pixel nicht über einstimmen und so eine andere Farbe vorweisen.\n",
        "\n",
        "Nachdem nun das Vergleichshistogram erstellt wurde, können die eingehenden Videoframes anhand dieses Histogramms analysiert werden.\n",
        "Zu aller erst müssen wir jedoch das eingehende Videoframe mittels `cv2.cvtColor` in den HSV Farbraum konvertieren. Anschließend lässt sich dieses Frame mittels des vorher ermittelnden Histogramms maskieren. Hierzu bietet sich die `cv2.calcBackProject` Funktion an, welches das zu maskierende Frame, die zu betrachtenden Farbkanäle, das ermittelte Histogramm, den im Histogramm zu betrachtenden Bereich sowie den Ausgabe-Skalierungsfaktor als übergabe erhält. \n",
        "\n",
        "Hier sehen wir unser unmaskiertes Frame.\n",
        "\n",
        "*Bild 8 vor dem Maskieren *\n",
        "\n",
        "*Bild 9 nach dem Maskieren*\n",
        "\n",
        "Hier sehen wir unser Frame, nachdem es maskiert wurde. Hier zu sehen sind jetzt nur noch schwarze und weiße bereiche. Die schwarzen Bereiche kennzeichnen die Bereiche, die nicht mit dem Histogramm übereinstimmenden Pixel enthielten, die weißen genau das Gegenteil.\n",
        "\n",
        "Nun haben wir eine sehr feine Maske mit den übereinstimmenden Bereichen. Allerdings benötigen wir einen gröberen Bereich um die Hand genau zu maskieren. Für diesen zweck vergrößern wir die erkannten Pixel innerhalb der Maskierung mittels der `cv2.filter2D` Funktion.\n",
        "\n",
        "*Bild 10 nach Filter2D*\n",
        "\n",
        "Anschließend filtern wir mittels eines Threshholds geringere Übereinstimmungen mittels `cv2.threshhold` heraus.\n",
        "\n",
        "*Bild 11 WEITER MIT REDUCE NOISE in evaluateFrame*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzTsnB54Ly-h"
      },
      "source": [
        "##Verbesserung der Fingerspitzenerkennung und Ausblick auf wiederverwendung einer KI\n",
        "\n",
        "Zunächst ist die Idee, dass wir fehlgesetzte Punkte der Fingerspitze (Bild 12) als Rauschen identifizieren und so ignorieren können. Dies soll der Erkennbarkeit der Fingerspitze steigern und die Anzahl der falsch-positive Punkte verringern.\n",
        "\n",
        "Dies ist umzusetzen indem man erst einen (rauschbehafteten) Satz an Punkten sammelt, von dort aus deren gemeinsamen Mittelpunkt errechnet und zuletzt alle weiteren Punkte, die nicht in einem gewissen Radius um diesen Mittelpunkt liegen, ignoriert. Dadurch erreicht man innerhalb weniger dutzend Bilder einen Rauschfreien Punktesatz\n",
        "\n",
        "![alt Text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/25.11.20/Finger_mit_Rauschen_Durch_Fehlgesetzte_Punkte.png)\n",
        "\n",
        "**Bild 12:** Finger mit rauschenden Punkten\n",
        "\n",
        "![alt Text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/25.11.20/Finger_Ohne_Rauschen.png)\n",
        "\n",
        "**Bild 13:** Finger ohne rauschende Punkte\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38qq9cE3yECl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}