{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "InteraktiveCamera.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/blob/main/Documentation/InteraktiveCamera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl34ucqPSTUC"
      },
      "source": [
        "# **Grundlegende Idee**\n",
        "Die Idee ist ein Zwei-Kamera-System zu einem Ein-Kamera-System umzugestalten. Dabei soll die erste Kamera auf einen Sprecher und die zweite Kamera auf ein Blatt Papier gerichtet sein. Sobald der Sprecher nun seinen Zeigefinger auf das Papier legt, wird dieses Kamerabild als Bild in Bild (pip) der ersten Kamerasicht angezeigt.\n",
        "\n",
        "Ein alternativer und womöglich attraktiverer Anwendungszeck ist, das System als Ein-Kamera-System in Verbindung mit einer Bildschirmübertragung zu verwenden: \n",
        "Studenten, wie beispielsweise Tutoren, haben oftmals den Bedarf daran etwas graphisch darstellen zu wollen, falls etwas in einer Präsentation den Zuhörern nicht ganz klar geworden ist. In einem Seminarraum ist dies mit einer Tafel schnell umgesetzt, in der jetztigen Corona Pandemie von 2020, gestaltet sich dies für viele ohne benötigtes Equipment wie einem Grafiktablett eher schwer.\n",
        "So liegt es nahe Papier und Stift anstelle des Grafiktablets zu verwenden und dieses Bild als pip in der Bildschirmübertragung anzuzeigen.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASWMfoeLbGr1"
      },
      "source": [
        "## Required Hardware\n",
        "\n",
        "The system should be as simple as possible and not require any special hardware. This means that a cell phone camera, which delivers a camera feed to the PC via apps such as [Droid Cam](http://www.dev47apps.com/) on Android and iOS, should be sufficient. Accordingly, it should also be possible to display this cell phone camera image within a screen transmission in order to support what is shown on the screen with hand drawings, for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MMYSMlbJMx"
      },
      "source": [
        "## Funktionen\n",
        "Wie bereits genannt wird der Nutzer zwei Bildinputs auswählen und festlegen müssen welche der Inputs das einzublendende Bild enthält. Dem Nutzer soll es möglich sein durch Zeigen mit dem Zeigefinger die Kamera zu Aktivieren und den Zeigefinger als Mittelpunkt des Bildes zu sehen. Durch eine \"Raus-Zoom\" Geste wie man sie von Smartphones kennt, wird ein digitaler Zoom ausführbar sein."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VvNiAGPbD6u"
      },
      "source": [
        "## Programmcode\n",
        "Jeglicher Programmcode ist in dem folgende Repository zu finden:\n",
        "[Medienverarbeitung Gruppe E - Interactive Camera](https://github.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2TPDKR8bXAx"
      },
      "source": [
        "## Vorgehen im Projekt\n",
        "Das Projekt wird als Projekttagebuch dokumentiert. Hier werden alle Ideen, Ansätze und Ergebnisse festgehalten. Zu Abschluss des Projekts wird das Gesamtergebnis noch einmal gesondert und aufbereitet präsentiert. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thBxNuIcqqI"
      },
      "source": [
        "# Projekttagebuch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxZ5eNO1c03y"
      },
      "source": [
        "## Erster Ansatz und Ergebnisse \n",
        "*11.11.20*\n",
        "\n",
        "Zu erst kümmern wir uns um die Erkennung eines zeigenden Zeigefingers. Dafür trainieren wir ein Keras Modell mit Bildern, welche zeigende Zeigefinger (Bild 1), weiße (karrierte, linierte, blanko) Papiere (Bild 2) und andere Gesten wie flache Hände und Ähnliches (Bild 3) unterscheiden können soll.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/11.11.20/zu_unterscheidende_Objekte.png)\n",
        "\n",
        "Das damit antrainierte Modell erkennt das gezeigte Bild und sortiert es der Rubrik ein, welche mit der höchsten Konfidenz übereinstimmt.\n",
        "\n",
        "Die Verarbeitung des eingehenden Videostreams wird durch die Python-Bibliothek \"[*OpenCV*](https://pypi.org/project/opencv-python/)\" ermöglicht.\n",
        "\n",
        "Durch diesen Ansatz konnten bisher erste Erfolge und enstehende Probleme verzeichnet werden\n",
        "\n",
        "###Probleme\n",
        "Es bisher Störfaktoren wie eine Computer-Maus (Bild 6) fälschlicherweise als ein Zeigen erkannt. Jedoch war es auch möglich in einer kontrollierten Umgebung einen Zeigefinger (Bild 4) von einem Blatt Papier (Bild 5) zu unterscheiden.\n",
        "![alt text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/11.11.20/Ergebnisse.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVF_fn3fwpQZ"
      },
      "source": [
        "##Verwurf vorheriger Ergebnisse und entdeckung neuer Technologien\n",
        "*18.11.2020*\n",
        "\n",
        "Eine Objekterkennung durch ein neurales Netzwerk bedarf einer ausgesprochen Großen Datenmenge, welche kuriert wurden, um sie auf Relevante objekte zu reduzieren. Dies heißt in unserem Fall, dass wir einen für zwei Personen nicht erzeugbaren Datensatz an zeigenden Fingern und Hintergrundbildern benötigen würden, um akkurate Ergebnise zu erziehlen. Daher brauchen wir eine Alternative, welche mit einem realistischen Arbeitsaufwand zu bewältigen ist.\n",
        "\n",
        "Nun stellt sich die Frage: Was macht einen zeigenden Finger aus? Oder noch simpler: Was macht eine Hand aus?\n",
        "\n",
        "Wir als Menschen erkennen Objekte auf Distanzen primär über Form und Farbe. So entsteht die Idee, dass ein Computer ein Bild nach der Handfarbe eines Nutzers absuchen und diesen Bereich isolieren kann. Dies ist in \"[*OpenCV*](https://pypi.org/project/opencv-python/)\" über sogenannte Histogramme möglich. \n",
        "Die Grundlegende Idee ist die solche, dass anfangs die Hautfarbe des Nutzers durch eine simple Bildprobe ermittelt wird und so eine Hand (bzw. etwas Hautfarbenes) in einem Bild erkannt werden kann. Dabei besteht die Annahme, dass auch nur *eine* Hand im Bild zu sehen ist und keine weiteren mit der Hautfarbe übereinstimmenden Gegenstände im Bild sind.\n",
        "\n",
        "![alt Text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/18.11.20/Messung_Hautfarbe.png \"Bild 7: Messpunkte auf Hand\")\n",
        "\n",
        "**Bild 7:** Messpunkte auf Hand\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In Bild 7 ist eine solche Messung zu sehen. Die dabei markierten Rechtecke werden zu einem Bild zusammengefügt und aus diesem entstehenden Bild ein Histogram errechnet. Hier zu sehen ist, wie diese Regionen markiert und für die Messung genutzt werden:\n",
        "\n",
        "\n",
        "```python\n",
        "def drawMeasuringRectangles(frame):\n",
        "    \"\"\"Draws 'amountOfMeasuringRectangles' Rectangles on the given frame and returns the modified image\"\"\"\n",
        "    rows, cols, dontCare = frame.shape\n",
        "    global amountOfMeasuringRectangles, xCoordinatesOfMeasuringRectangles_topLeft, yCoordinatesOfMeasuringRectangles_topLeft, xCoordinatesOfMeasuringRectangles_bottomRight, yCoordinatesOfMeasuringRectangles_bottomRight\n",
        "\n",
        "    # position messure points of hand histogram\n",
        "    xCoordinatesOfMeasuringRectangles_topLeft = np.array(\n",
        "        [6 * rows / 20, 6 * rows / 20, 6 * rows / 20, 9 * rows / 20, 9 * rows / 20, 9 * rows / 20, 12 * rows / 20,\n",
        "         12 * rows / 20, 12 * rows / 20], dtype=np.uint32)\n",
        "\n",
        "    yCoordinatesOfMeasuringRectangles_topLeft = np.array(\n",
        "        [9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20,\n",
        "         10 * cols / 20, 11 * cols / 20], dtype=np.uint32)\n",
        "\n",
        "    # define shape of drawn small rectangles | here 10x10\n",
        "    xCoordinatesOfMeasuringRectangles_bottomRight = xCoordinatesOfMeasuringRectangles_topLeft + 10\n",
        "    yCoordinatesOfMeasuringRectangles_bottomRight = yCoordinatesOfMeasuringRectangles_topLeft + 10\n",
        "\n",
        "    # draw calculated rectangles\n",
        "    for i in range(amountOfMeasuringRectangles):\n",
        "        cv2.rectangle(frame,\n",
        "                      (yCoordinatesOfMeasuringRectangles_topLeft[i], xCoordinatesOfMeasuringRectangles_topLeft[i]),\n",
        "                      (yCoordinatesOfMeasuringRectangles_bottomRight[i],\n",
        "                       xCoordinatesOfMeasuringRectangles_bottomRight[i]),\n",
        "                      (0, 255, 0), 1)\n",
        "\n",
        "    return frame\n",
        "\n",
        "  def createHandHistogram(frame):\n",
        "    global xCoordinatesOfMeasuringRectangles_topLeft, yCoordinatesOfMeasuringRectangles_topLeft\n",
        "\n",
        "    # convert cv2 bgr colorspace to hsv colorspace for easier handling\n",
        "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    # create new blank Region Of Interest matrix/image\n",
        "    roi = np.zeros([90, 10, 3], dtype=hsv_frame.dtype)\n",
        "\n",
        "    # fill ROI with the sample rectangles\n",
        "    for i in range(amountOfMeasuringRectangles):\n",
        "        roi[i * 10: i * 10 + 10, 0: 10] = hsv_frame[xCoordinatesOfMeasuringRectangles_topLeft[i]:\n",
        "                                                    xCoordinatesOfMeasuringRectangles_topLeft[i] + 10,\n",
        "                                          yCoordinatesOfMeasuringRectangles_topLeft[i]:\n",
        "                                          yCoordinatesOfMeasuringRectangles_topLeft[i] + 10]\n",
        "\n",
        "    # create a Hand histogram and normalize it\n",
        "    hand_hist = cv2.calcHist([roi], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
        "\n",
        "    # remove noise and retun\n",
        "    return cv2.normalize(hand_hist, hand_hist, 0, 255, cv2.NORM_MINMAX)\n",
        "```\n",
        "Das daraus entstehende Histogram wird verwendet, um auf das gegebene Bild bzw. den Videoframe einen *Back Projection* anzuwenden. Dabei stimmen hellere (weißere) Pixel mehr mit dem errechneten Histogram überein, während dunklere (schwarze) Pixel nicht über einstimmen und so eine andere Farbe vorweisen. Das daraus resultierende Bild ist in *Bild 8* zu sehen\n",
        "\n",
        "![altText](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/18.11.20/Hand_Nach_Histogram_BackProjection.png)\n",
        "\n",
        "**Bild 8:** Hand nach `cv2.calcBackProject( [...] )`\n",
        "\n",
        "Offensichtlich ist dieses Bild noch sehr verrauscht und zu feingranular, als dass man daraus einfach einen nützlichen Informationsgewinn erzielen könnte. Aufgrund dessen werden mit einer simplen *Threshhold* Operation alle Pixel als unrelevant erachtet, welche einen zu geringen Weißwert aufweisen und so am ehesten einen falsch-positiven Pixel darstellen. Diese Übriggebliebenen Pixel werden abschließend jeweils mit einem weißen Kreis maskiert, um die verlorenen Pixel wett zu machen und das bild in ein simples schwarzweiß Bild, wie in Bild 9 zu sehen ist, umzuwandeln.\n",
        "\n",
        "![alttext](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/18.11.20/SchwarzWei%C3%9F_Histogram_Backprojection.png)\n",
        "\n",
        "**Bild 9:** Schwarzweiß Bild nach `cv2.threshhold( [...] )`\n",
        "\n",
        "Diese Maske wird nun Abschließend genutzt, um eine im Optimalfall konvexe Hülle um die Hand zu legen. Mithilfe dieser Hülle berechnen wir nun den Mittelpunkt der Hand und zeichnen diesen ein. Dies ist der lilane Punkt in Bild 10.  \n",
        "\n",
        "```python\n",
        "#Beispielcode zum berechnen eines Mittelpunktes anhand von Konturen\n",
        "[...]\n",
        "contourList = getContoursFromMaskedImage(maskedHistogramImage)\n",
        "  if contourList:\n",
        "    maxCont = max(contourList, key=cv2.contourArea)\n",
        "    centerOfMaxCont = getCenterCoordinatesOfContour(maxCont)\n",
        "[...]\n",
        "\n",
        "\n",
        "def getContoursFromMaskedImage(maskedHistogramImage):\n",
        "    \"\"\"Returns the contours of a given masked Image\"\"\"\n",
        "    grayscaledMaskedHistogramImage = cv2.cvtColor(maskedHistogramImage, cv2.COLOR_BGR2GRAY)\n",
        "    ret, thresh = cv2.threshold(grayscaledMaskedHistogramImage, 0, 255, 0)\n",
        "    cont, hierarchyDontCare = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    return cont\n",
        "``` \n",
        "\n",
        "Zeigt diese Hand nun mit dem Finger auf etwas, so ist dies durch einen Störung der konvexen Hülle, als ein sogenanter *konvexer Defekt*, erkennbar.  Wir nutzen diesen Defekt, um mithilfe des Mittelpunkts der Hand den am weitesten sich auf der Kontur befindlichen Punkt zu ermitteln. Dabei besteht die Annahme, dass dieser Punkt die Fingerspitze repräsentiert. Der folgende Code zeigt eine solche Punktermittlung:\n",
        "\n",
        "```python\n",
        "hull = cv2.convexHull(maxCont, returnPoints=False)\n",
        "            defects = cv2.convexityDefects(maxCont, hull)\n",
        "            farthestPoint = getFarthestPointFromContour(defects, maxCont, centerOfMaxCont)\n",
        "\n",
        "def getFarthestPointFromContour(defects, contour, centroid):\n",
        "    \"\"\"Returns the farthest point from a given centerpoint on a contour using defects\"\"\"\n",
        "    if defects is not None and centroid is not None:\n",
        "        s = defects[:, 0][:, 0]\n",
        "        cx, cy = centroid\n",
        "\n",
        "        x = np.array(contour[s][:, 0][:, 0], dtype=np.float)\n",
        "        y = np.array(contour[s][:, 0][:, 1], dtype=np.float)\n",
        "\n",
        "        xp = cv2.pow(cv2.subtract(x, cx), 2)\n",
        "        yp = cv2.pow(cv2.subtract(y, cy), 2)\n",
        "        dist = cv2.sqrt(cv2.add(xp, yp))\n",
        "\n",
        "        dist_max_i = np.argmax(dist)\n",
        "\n",
        "        if dist_max_i < len(s):\n",
        "            farthest_defect = s[dist_max_i]\n",
        "            return tuple(contour[farthest_defect][0])\n",
        "        else:\n",
        "            return None\n",
        "```\n",
        "In Bild 10 sehen wir wie diese Punkte (hier gelb) genutzt werden um sie als Fingerspitze anzuzeigen. \n",
        "\n",
        "Jedoch durch diese Operationen auch rauschende Pixel wie in Bild 8 zu sehen ist hin und wieder immernoch als Teil der Hand erkannt und sorgen so für Probleme bei der Erkennung von konvexen Defekten. Dieses Problem gilt es zu lösen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzTsnB54Ly-h"
      },
      "source": [
        "##Verbesserung der Fingerspitzenerkennung und Ausblick auf wiederverwendung einer KI\n",
        "\n",
        "Zunächst ist die Idee, dass wir fehlgesetzte Punkte der Fingerspitze (Bild 10) als Rauschen identifizieren und so ignorieren können. Dies soll der Erkennbarkeit der Fingerspitze steigern und die Anzahl der falsch-positive Punkte verringern.\n",
        "\n",
        "Dies ist umzusetzen indem man erst einen (rauschbehafteten) Satz an Punkten sammelt, von dort aus deren gemeinsamen Mittelpunkt errechnet und zuletzt alle weiteren Punkte, die nicht in einem gewissen Radius um diesen Mittelpunkt liegen, ignoriert. Dadurch erreicht man innerhalb weniger dutzend Bilder einen Rauschfreien Punktesatz wie diesem in Bild 11\n",
        "\n",
        "![alt Text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/25.11.20/Finger_mit_Rauschen_Durch_Fehlgesetzte_Punkte.png)\n",
        "\n",
        "**Bild 10:** Finger mit rauschenden Punkten\n",
        "\n",
        "![alt Text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/25.11.20/Finger_Ohne_Rauschen.png)\n",
        "\n",
        "**Bild 11:** Finger ohne rauschende Punkte\n",
        "\n",
        "\n",
        "Da nun die Fingerspitzenerkennung für alle weiteren Nutzungszwecke ausreichend funktioniert, gilt es nun das gleiche für die Erkennung des Handmittelpunktes durchzuführen. Die Rauschunterdrückung läuft auch hier analog zur vorherigen ab.\n",
        "\n",
        "\n",
        "\n",
        "## Hinzufügen einer einfachen Gui, um den zu verwendenen Monitor und die Kamera auszuwählen\n",
        "\n",
        "Damit der Nutzer später auch zwischen verschiedenen Monitoren und ggf. Kameras einfach wechseln kann, ohne das Programm anpassen zu müssen, wurde von uns eine SImple GUI entwicklet. Diese stellt zwei Dropdown Menüs bereit, welche wiederum alle angeschlossenen Monitore und Kameras auflisten und zwischen denen beliebig gewechselt werden kann.\n",
        "\n",
        "Um diese möglichst einfach zu implementieren, verwenden wir eine Bibliothek namens TKInter. Diese bietet eine recht intuitive und einfache Möglichkeit, diverse Bedienelemente in einem Fenster anzuordnen und zu verwalten.\n",
        "\n",
        "**Bild 12:** DropDown Menüs\n",
        "\n",
        "##Ausblick auf die KI\n",
        "\n",
        "Ursprünglich haben wir Tensorflow verwendet, um einen Zeigenden Finger von einem nichtzeigenden Finger zu unterscheiden. Jedoch stößt diese Herangehensweise schnell an seine Grenzen. Beispielsweise werden schwarze Hände nicht erkannt, da das Trainingsmaterial aus ausschließlich weißen Händen besteht. Jedoch ist es möglich durch die Verwendung von Bildern wie Bild 9 oder Bild 8 ein neutrales Grundbild zu schaffen. Das heißt, unter Verwendung dieser Bilder ist es egal welche Hautfarbe der Benutzer hat, was dazu führt, dass das Trainingsmaterial so auch neutraler gestaltet ist. Ziel dieser neutralität ist es die Handerkennung so zugänglicher für jeden zu machen.\n",
        "Außerdem wird es notwendig werden für eine Gestenerkennung eine KI zu verwenden, um den Arbeitsaufwand vergleichsweise gering zu halten.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqz3G_cU43rM"
      },
      "source": [
        "## \n",
        "*02.12.2020*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38qq9cE3yECl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx_PvGxD0qav"
      },
      "source": [
        "## Experimente mit Multithreading und weitere Verbesserung der Erkennung\n",
        "*09.12.2020*\n",
        "\n",
        "Als nun die GUI implementiert war und auch das Monitor und Kamerabild in einem Fenster und weiteren Debug-Fenstern angezeigt wurde, mussten wir feststellen, dass darunter die Performance litt.\n",
        "Wir stellen fest, das die Bildwiederholrate durch die ganzen berechnungen und aktualisierungen unter 5 Bilder pro Sekunde fiel. Dies sorgte nicht nur für ein extrem ruckeliges Bild, sondern auch für eine langsamere Verarbeitung der nächsten Bilder, da das auslesen des Kamerabildes und des Monitors viel Zeit beanspruchte. Wir haben uns gedanken dazu gemacht, wie wir dieses Problem beheben können. Einfach einen immer leistungsstärkeren Computer zu verwenden kommt natürlich nicht in Frage. Also mussten wir uns was überlegen, was die benötigte Performance senkt, m die Sotware auch auf nicht so performanten Computern lauffähig zu machen, gleichzeitig aber auch sicherstellen, das alle Funktionen wie gewohnt funktionieren. Nach einer kurzen Internetrecherche kamen wir auf die Idee, einige der Aufgaben in eigene Threads auszulagern, um den Mainthread nicht unnötig zu belasten und somit ein paar Bilder fro Sekunde gutmachen können. Die einfachste Möglichkeit, die uns eingefallen ist war, das Auslesen des Kamerabildes und des Monitors in einen eigenen Thread auszulagern und das Bild in Variablen innerhalb des Objektes verfügbar zu machen um es kurz vor der Bearbeitung einfach abgreifen zu können. Dies beschwerte uns bereits einen Performancezuwachs von knapp 10-20 Bildern pro Sekunde. Dieser Unterscheid war sehr deutlich im angezeigten Videofeed dadurch zu beobachten, als dass das Video viel flüssiger war. Weitere Versuche, auch die Verarbeitung des Bildes in einen eigenen Thread auszulagern und nurnoch dann das Fenster zu aktualisieren, wenn das Bild erfolgreich verarbeitet wurde zeigten leider keine große Verbesserung.\n",
        "\n",
        "Im Folgenden Beispiel sieht man, wie das Kamerabild automatisch immer wieder eingelesen wird wenn der Thread mittels start() gestartet wurde und innerhalb des Objektes gespeichert wird. Dies lässt sich bei bedarf einfach abrufen.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JxoSkW383xh"
      },
      "source": [
        "class CameraGrabber(object):\n",
        "    \"\"\"\n",
        "    Reads the Current Camera-feed in another Thread and Stores it for easy Access\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, src, width=1280, height=720):\n",
        "        \"\"\"\n",
        "        Initialize a new CameraGrabber\n",
        "        :param src: CameraIndex from mss\n",
        "        :param width: Scaled Output Image width\n",
        "        :param height: Scaled Output Image hight\n",
        "        \"\"\"\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "        # Grab Camera Image, Resize, Convert and Store it\n",
        "        self.stream = cv2.VideoCapture(src)\n",
        "        (self.grabbed, img) = self.stream.read()\n",
        "        self.picture = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        self.stopped = False\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"\n",
        "        Starts another Thread for its own get-Method, to grab the Image out of Mainloop\n",
        "        :return:  Optional: The Own Object to create, start the Thread and save the Object at the same Time\n",
        "        \"\"\"\n",
        "        Thread(target=self.get, args=()).start()\n",
        "        return self\n",
        "\n",
        "    def setSrc(self, src):\n",
        "        \"\"\"\n",
        "        Re-Sets the Camera Input Source Index of mss\n",
        "        :param src: The new Camera Index\n",
        "        \"\"\"\n",
        "        self.stream = cv2.VideoCapture(src)\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Grabs the current Camera Image, Resize and stores it\n",
        "        \"\"\"\n",
        "        while not self.stopped:\n",
        "            if not self.grabbed:\n",
        "                self.stop()\n",
        "            else:\n",
        "                (self.grabbed, img) = self.stream.read()\n",
        "                self.picture = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"\n",
        "        Stops the CameraGrabber-Get-Thread started by the start-Method\n",
        "        \"\"\"\n",
        "        self.stopped = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqnq1IP-82XY"
      },
      "source": [
        "Gleichzeitig wurde daran gararbeitet, die Fingererkennung noch mehr zu verfeinern. Hierzu wurden vorhandene Algorithmen weiter optimiert und Fehler wurden behoben, die das Program einfrieren ließen, wenn bestimmte Aktionen getätigt wurden. Auch wurde ein Fehler behoben, der dafür gesorgt hat, das Bilder nicht richtig übergeben wurden und somit falsch im Fenster angezeigt wurden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl4HteMB8ypX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}