{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "InteraktiveCamera.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/blob/main/Documentation/InteraktiveCamera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl34ucqPSTUC"
      },
      "source": [
        "# **Grundlegende Idee**\n",
        "Die Idee ist ein Zwei-Kamera-System zu einem Ein-Kamera-System umzugestalten. Dabei soll die erste Kamera auf einen Sprecher und die zweite Kamera auf ein Blatt Papier gerichtet sein. Sobald der Sprecher nun seinen Zeigefinger auf das Papier legt, wird dieses Kamerabild als Bild in Bild (pip) der ersten Kamerasicht angezeigt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASWMfoeLbGr1"
      },
      "source": [
        "## Benötigte Hardware\n",
        "Das System soll so einfach wie möglich sein und keine spezielle Hardware benötigen. Das heißt eine Handykamera, welche über Apps wie beispielsweise [Droid Cam](http://www.dev47apps.com/) auf Android einen Kamerafeed an den PC liefern sollen schon ausreichen. Dementsprechend soll es auch möglich sein dieses Handykamerabild innerhalb einer Bildschirmübertragung anzuzeigen, um so gezeigtes auf dem Bildschirm durch beispielsweise Handzeichnungen zu unterstützen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MMYSMlbJMx"
      },
      "source": [
        "## Funktionen\n",
        "Wie bereits genannt wird der Nutzer zwei Bildinputs auswählen und festlegen müssen welche der Inputs das einzublendende Bild enthält. Dem Nutzer soll es möglich sein durch Zeigen mit dem Zeigefinger die Kamera zu Aktivieren und den Zeigefinger als Mittelpunkt des Bildes zu sehen. Durch eine \"Raus-Zoom\" Geste wie man sie von Smartphones kennt, wird ein digitaler Zoom ausführbar sein."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VvNiAGPbD6u"
      },
      "source": [
        "## Programmcode\n",
        "Jeglicher Programmcode ist in dem folgende Repository zu finden:\n",
        "[Medienverarbeitung Gruppe E - Interactive Camera](https://github.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2TPDKR8bXAx"
      },
      "source": [
        "## Vorgehen im Projekt\n",
        "Das Projekt wird als Projekttagebuch dokumentiert. Hier werden alle Ideen, Ansätze und Ergebnisse festgehalten. Zu Abschluss des Projekts wird das Gesamtergebnis noch einmal gesondert und aufbereitet präsentiert. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thBxNuIcqqI"
      },
      "source": [
        "# Projekttagebuch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxZ5eNO1c03y"
      },
      "source": [
        "## Erster Ansatz und Ergebnisse \n",
        "*11.11.20*\n",
        "\n",
        "Zu erst kümmern wir uns um die Erkennung eines zeigenden Zeigefingers. Dafür trainieren wir ein Keras Modell mit Bildern, welche zeigende Zeigefinger (Bild 1), weiße (karrierte, linierte, blanko) Papiere (Bild 2) und andere Gesten wie flache Hände und Ähnliches (Bild 3) unterscheiden können soll.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/11.11.20/zu_unterscheidende_Objekte.png)\n",
        "\n",
        "Das damit antrainierte Modell erkennt das gezeigte Bild und sortiert es der Rubrik ein, welche mit der höchsten Konfidenz übereinstimmt.\n",
        "\n",
        "Die Verarbeitung des eingehenden Videostreams wird durch die Python-Bibliothek \"[*OpenCV*](https://pypi.org/project/opencv-python/)\" ermöglicht.\n",
        "\n",
        "Durch diesen Ansatz konnten bisher erste Erfolge und enstehende Probleme verzeichnet werden\n",
        "\n",
        "###Probleme\n",
        "Es bisher Störfaktoren wie eine Computer-Maus (Bild 6) fälschlicherweise als ein Zeigen erkannt. Jedoch war es auch möglich in einer kontrollierten Umgebung einen Zeigefinger (Bild 4) von einem Blatt Papier (Bild 5) zu unterscheiden.\n",
        "![alt text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/11.11.20/Ergebnisse.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVF_fn3fwpQZ"
      },
      "source": [
        "##Verwurf vorheriger Ergebnisse und entdeckung neuer Technologien\n",
        "*18.11.2020*\n",
        "\n",
        "Eine Objekterkennung durch ein neurales Netzwerk bedarf einer ausgesprochen Großen Datenmenge, welche kuriert wurden, um sie auf Relevante objekte zu reduzieren. Dies heißt in unserem Fall, dass wir einen für zwei Personen nicht erzeugbaren Datensatz an zeigenden Fingern und Hintergrundbildern benötigen würden, um akkurate Ergebnise zu erziehlen. Daher brauchen wir eine Alternative, welche mit einem realistischen Arbeitsaufwand zu bewältigen ist.\n",
        "\n",
        "Nun stellt sich die Frage: Was macht einen zeigenden Finger aus? Oder noch simpler: Was macht eine Hand aus?\n",
        "\n",
        "Wir als Menschen erkennen Objekte auf Distanzen primär über Form und Farbe. So entsteht die Idee, dass ein Computer ein Bild nach der Handfarbe eines Nutzers absuchen und diesen Bereich isolieren kann. Dies ist in \"[*OpenCV*](https://pypi.org/project/opencv-python/)\" über sogenannte Histogramme möglich. \n",
        "Die Grundlegende Idee ist die solche, dass anfangs die Hautfarbe des Nutzers durch eine simple Bildprobe ermittelt wird und so eine Hand (bzw. etwas Hautfarbenes) in einem Bild erkannt werden kann. Dabei besteht die Annahme, dass auch nur *eine* Hand im Bild zu sehen ist und keine weiteren mit der Hautfarbe übereinstimmenden Gegenstände im Bild sind.\n",
        "\n",
        "![alt Text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/18.11.20/Messung_Hautfarbe.png \"Bild 7: Messpunkte auf Hand\")\n",
        "\n",
        "**Bild 7:** Messpunkte auf Hand\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In Bild 7 ist eine solche Messung zu sehen. Die dabei markierten Rechtecke werden zu einem Bild zusammengefügt und aus diesem entstehenden Bild ein Histogram errechnet. Hier zu sehen ist, wie diese Regionen markiert und für die Messung genutzt werden:\n",
        "\n",
        "\n",
        "```python\n",
        "def drawMeasuringRectangles(frame):\n",
        "    \"\"\"Draws 'amountOfMeasuringRectangles' Rectangles on the given frame and returns the modified image\"\"\"\n",
        "    rows, cols, dontCare = frame.shape\n",
        "    global amountOfMeasuringRectangles, xCoordinatesOfMeasuringRectangles_topLeft, yCoordinatesOfMeasuringRectangles_topLeft, xCoordinatesOfMeasuringRectangles_bottomRight, yCoordinatesOfMeasuringRectangles_bottomRight\n",
        "\n",
        "    # position messure points of hand histogram\n",
        "    xCoordinatesOfMeasuringRectangles_topLeft = np.array(\n",
        "        [6 * rows / 20, 6 * rows / 20, 6 * rows / 20, 9 * rows / 20, 9 * rows / 20, 9 * rows / 20, 12 * rows / 20,\n",
        "         12 * rows / 20, 12 * rows / 20], dtype=np.uint32)\n",
        "\n",
        "    yCoordinatesOfMeasuringRectangles_topLeft = np.array(\n",
        "        [9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20, 10 * cols / 20, 11 * cols / 20, 9 * cols / 20,\n",
        "         10 * cols / 20, 11 * cols / 20], dtype=np.uint32)\n",
        "\n",
        "    # define shape of drawn small rectangles | here 10x10\n",
        "    xCoordinatesOfMeasuringRectangles_bottomRight = xCoordinatesOfMeasuringRectangles_topLeft + 10\n",
        "    yCoordinatesOfMeasuringRectangles_bottomRight = yCoordinatesOfMeasuringRectangles_topLeft + 10\n",
        "\n",
        "    # draw calculated rectangles\n",
        "    for i in range(amountOfMeasuringRectangles):\n",
        "        cv2.rectangle(frame,\n",
        "                      (yCoordinatesOfMeasuringRectangles_topLeft[i], xCoordinatesOfMeasuringRectangles_topLeft[i]),\n",
        "                      (yCoordinatesOfMeasuringRectangles_bottomRight[i],\n",
        "                       xCoordinatesOfMeasuringRectangles_bottomRight[i]),\n",
        "                      (0, 255, 0), 1)\n",
        "\n",
        "    return frame\n",
        "\n",
        "  def createHandHistogram(frame):\n",
        "    global xCoordinatesOfMeasuringRectangles_topLeft, yCoordinatesOfMeasuringRectangles_topLeft\n",
        "\n",
        "    # convert cv2 bgr colorspace to hsv colorspace for easier handling\n",
        "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    # create new blank Region Of Interest matrix/image\n",
        "    roi = np.zeros([90, 10, 3], dtype=hsv_frame.dtype)\n",
        "\n",
        "    # fill ROI with the sample rectangles\n",
        "    for i in range(amountOfMeasuringRectangles):\n",
        "        roi[i * 10: i * 10 + 10, 0: 10] = hsv_frame[xCoordinatesOfMeasuringRectangles_topLeft[i]:\n",
        "                                                    xCoordinatesOfMeasuringRectangles_topLeft[i] + 10,\n",
        "                                          yCoordinatesOfMeasuringRectangles_topLeft[i]:\n",
        "                                          yCoordinatesOfMeasuringRectangles_topLeft[i] + 10]\n",
        "\n",
        "    # create a Hand histogram and normalize it\n",
        "    hand_hist = cv2.calcHist([roi], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
        "\n",
        "    # remove noise and retun\n",
        "    return cv2.normalize(hand_hist, hand_hist, 0, 255, cv2.NORM_MINMAX)\n",
        "```\n",
        "Das daraus entstehende Histogram wird verwendet, um auf das gegebene Bild bzw. den Videoframe einen Graustufenfilter anzuwenden. Dabei stimmen hellere (weißere) Pixel mehr mit dem errechneten Histogram überein, während dunklere (schwarze) Pixel nicht über einstimmen und so eine andere Farbe vorweisen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38qq9cE3yECl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}