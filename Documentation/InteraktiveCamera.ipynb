{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "InteraktiveCamera.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/blob/main/Documentation/InteraktiveCamera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl34ucqPSTUC"
      },
      "source": [
        "# **Grundlegende Idee**\n",
        "Die Idee ist ein Zwei-Kamera-System zu einem Ein-Kamera-System umzugestalten. Dabei soll die erste Kamera auf einen Sprecher und die zweite Kamera auf ein Blatt Papier gerichtet sein. Sobald der Sprecher nun seinen Zeigefinger auf das Papier legt, wird dieses Kamerabild als Bild in Bild (pip) der ersten Kamerasicht angezeigt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASWMfoeLbGr1"
      },
      "source": [
        "## Benötigte Hardware\n",
        "Das System soll so einfach wie möglich sein und keine spezielle Hardware benötigen. Das heißt eine Handykamera, welche über Apps wie beispielsweise [Droid Cam](http://www.dev47apps.com/) auf Android einen Kamerafeed an den PC liefern sollen schon ausreichen. Dementsprechend soll es auch möglich sein dieses Handykamerabild innerhalb einer Bildschirmübertragung anzuzeigen, um so gezeigtes auf dem Bildschirm durch beispielsweise Handzeichnungen zu unterstützen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_MMYSMlbJMx"
      },
      "source": [
        "## Funktionen\n",
        "Wie bereits genannt wird der Nutzer zwei Bildinputs auswählen und festlegen müssen welche der Inputs das einzublendende Bild enthält. Dem Nutzer soll es möglich sein durch Zeigen mit dem Zeigefinger die Kamera zu Aktivieren und den Zeigefinger als Mittelpunkt des Bildes zu sehen. Durch eine \"Raus-Zoom\" Geste wie man sie von Smartphones kennt, wird ein digitaler Zoom ausführbar sein."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VvNiAGPbD6u"
      },
      "source": [
        "## Programmcode\n",
        "Jeglicher Programmcode ist in dem folgende Repository zu finden:\n",
        "[Medienverarbeitung Gruppe E - Interactive Camera](https://github.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2TPDKR8bXAx"
      },
      "source": [
        "## Vorgehen im Projekt\n",
        "Das Projekt wird als Projekttagebuch dokumentiert. Hier werden alle Ideen, Ansätze und Ergebnisse festgehalten. Zu Abschluss des Projekts wird das Gesamtergebnis noch einmal gesondert und aufbereitet präsentiert. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thBxNuIcqqI"
      },
      "source": [
        "# Projekttagebuch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxZ5eNO1c03y"
      },
      "source": [
        "## Erster Ansatz und Ergebnisse \n",
        "*11.11.20*\n",
        "\n",
        "Zu erst kümmern wir uns um die Erkennung eines zeigenden Zeigefingers. Dafür trainieren wir ein Keras Modell mit Bildern, welche zeigende Zeigefinger (Bild 1), weiße (karrierte, linierte, blanko) Papiere (Bild 2) und andere Gesten wie flache Hände und Ähnliches (Bild 3).\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/zu_unterscheidende_Objekte.png)\n",
        "\n",
        "Das damit antrainierte Modell erkennt das gezeigte Bild und sortiert es der Rubrik ein, welche mit der höchsten Konfidenz übereinstimmt.\n",
        "\n",
        "Die Verarbeitung des eingehenden Videostreams wird durch die Python-Bibliothek \"[OpenCV](https://pypi.org/project/opencv-python/)\" ermöglicht.\n",
        "\n",
        "Durch diesen Ansatz konnten bisher erste Erfolge und enstehende Probleme verzeichnet werden\n",
        "\n",
        "###Probleme\n",
        "Es bisher Störfaktoren wie eine Computer-Maus (Bild 6) fälschlicherweise als ein Zeigen erkannt. Jedoch war es auch möglich in einer kontrollierten Umgebung einen Zeigefinger (Bild 4) von einem Blatt Papier (Bild 5) zu unterscheiden.\n",
        "![alt text](https://raw.githubusercontent.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/main/Documentation/Pictures/11.11.20/Ergebnisse.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdapu5Cdcz0A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}