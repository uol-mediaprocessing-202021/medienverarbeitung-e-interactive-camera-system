{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM90sasM/ZScSpoa7cGM+v8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/blob/main/Documentation/Documentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnKkxP6Qhodc"
      },
      "source": [
        "#Motivation\n",
        "In the current Corona pandemic, many people are tied to their own home offices and therefore have to do their work mostly with meager equipment. As a result, many students are also increasingly exposed to online tutoring, where one of the things they have to do is present solutions in tutorials. Some students may even be the tutor themselves and lead the lessons. Under normal circumstances, one would traditionally sit in a room with a projector and blackboard or whiteboard. Here, the latter two media help immensely to explain any questions by visually supporting what is being said. However, this technique is not easy to implement at home and requires technical equipment such as a graphics tablet or a document camera. The only problem is that not everyone has access to such technology and therefore the quality of the tutorial suffers from this lack.\n",
        "\n",
        "To counteract this, an interactive camera system presents itself as an attractive solution. This active camera system should be able to identify an outstretched index finger and zoom in on its tip in order to better display what is being shown. Furthermore, the zoom should be controlled by a gesture using a flat palm with an extended thumb, so that the entire control of the software, after an initial start, can be done hands-free.\n",
        "\n",
        "In the following, we will implement our interactive camera step by step and go into each aspect in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOsgx-5ShwKg"
      },
      "source": [
        "# Gesture recognition with Tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHLAoTMKiCni"
      },
      "source": [
        "## Tensorflow as our Machine-Learning Software\n",
        "Machine learning is an important part of our gesture recognition. The software available to us today is far more powerful than we needed in this case. This is also the case with TensorFlow. Nevertheless, we decided to use this software because it is easy to implement and no further knowledge is required for the initial setup. In addition, we are already provided with more in-depth information on the application of this in the course of the event."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V6q6g7xiE8v"
      },
      "source": [
        "## Aquire training data\n",
        "In order to achieve a high degree of consistency in the recognized gestures, we first need a large data set. We also have to think about in which part of the software the gesture recognition should take place. First of all, there are many possibilities. It would be possible to recognize the gesture before any processing of the image. However, this would cause a lot of problems. For example, we would have to take hundreds of pictures of each gesture with different lighting conditions, skin colors and backgrounds in order to achieve even a rudimentarily accurate result. Another possibility for gesture recognition would come after backprojection. Here, what is most important for gesture recognition has already been filtered out. The hand. At the same time we get an image that is only available in black and white and would not need the background, nor the skin color for training. However, there are still some artifacts to be seen, as certain areas of the image have a similar hue, but do not belong to the hand. Therefore, the best step would be the last step of the processing. Thresholding. As already described above, the occurring artifacts are filtered out during backprojection and the important areas are additionally highlighted.\n",
        "\n",
        "Now, to get as much data as possible, we collected over 1000 images per gesture. Since we don't have to pay attention to background, lighting conditions, or skin color, we were able to create the dataset very quickly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YGLEtzMieD1"
      },
      "source": [
        "### Live video capture of gestures we want to recognise\n",
        "To collect the data we had several options. On the one hand, the software itself could store the processed images and we would only have to sift through them once and sort out any inaccurate results. However, this would have a strong impact on the performance and slow down the creation of the data set.\n",
        "We considered it more useful to record the displayed output of the processed frame as a screen video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nogytwtQi5zg"
      },
      "source": [
        "## Prepare training data\n",
        "Since the created video cannot simply serve as training data in TensorFlow, they had to be further prepared by us beforehand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yClu5aEei9M9"
      },
      "source": [
        "### Converting the Videos to Images, cropping and resizing\n",
        "We converted the created videos into a sequence of images. In addition, we reduced the images to the relevant area for us. These images also had to be sifted by us afterwards in order to sort out errors. Since a tensor flow model works with an input of 224 x 224 images, we scaled our images to the same size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj7iGjofjC5v"
      },
      "source": [
        "## Training of the Model\n",
        "The training of the model could now be started. For this, there is a very helpful website (https://teachablemachine.withgoogle.com), which takes over the entire training of the model and provides suitable training methods depending on the use case. We chose the Image Classification model for our purpose.\n",
        "![TeachableMachine.com](https://github.com/uol-mediaprocessing-202021/medienverarbeitung-e-interactive-camera-system/blob/main/Documentation/Pictures/27.01.21/teachableMachine.jpg?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL-HKuRPjnBi"
      },
      "source": [
        "## Implementation of the Model\n",
        "The actual use of the code turned out to be very simple. After the model was trained, we were given the opportunity to take a simple example of the implementation of this Keras model in Python directly. This just had to be adapted a bit to our needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQEb7jFokWaJ"
      },
      "source": [
        "def getGesturePredictionFromTensorflow(frame, model):\n",
        "    if frame is None or model is None or type(frame) != np.ndarray or type(model) != tf.keras.Sequential:\n",
        "        return \"OTHER\"\n",
        "    h1 = frame.shape[0]\n",
        "    w1 = frame.shape[1]\n",
        "\n",
        "    # Create the array of the right shape to feed into the keras model\n",
        "    # The 'length' or number of images you can put into the array is\n",
        "    # determined by the first position in the shape tuple, in this case 1.\n",
        "    data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
        "\n",
        "    # Replace this with the path to your image\n",
        "    dimension = (224, 224)\n",
        "    image = cv2.resize(frame, dimension, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # turn the image into a numpy array\n",
        "    image_array = np.asarray(image)\n",
        "\n",
        "    # Normalize the image\n",
        "    normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
        "\n",
        "    # Load the image into the array\n",
        "    data[0] = normalized_image_array\n",
        "\n",
        "    # run the inference\n",
        "    prediction = model.predict(data)\n",
        "\n",
        "    # print(prediction)\n",
        "    predictionDictionary = {\n",
        "        \"LEFT\": prediction[0][0],\n",
        "        \"RIGHT\": prediction[0][1],\n",
        "        \"OTHER\": prediction[0][2]\n",
        "    }\n",
        "    global lastDetection, lastDetectionCount\n",
        "    detection = max(predictionDictionary.items(), key=operator.itemgetter(1))[0]\n",
        "    if lastDetection is None or lastDetection != detection:\n",
        "        lastDetection = detection\n",
        "        lastDetectionCount = 0\n",
        "    else:\n",
        "        lastDetectionCount += 1\n",
        "\n",
        "    return detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz3mc2YTjpPe"
      },
      "source": [
        "# GUI\n",
        "A software like ours, which is to be used as a collaborative tool, needs a GUI just because of a live preview. Therefore, we wanted to make it as simple and clear as possible. We also had to take into account that the software will be used on systems that have multiple cameras and screens. So we had to create a way to switch between the different monitors and cameras as easily as possible. This also without restarting the software.\n",
        "\n",
        "We also had to think about how to display multiple windows that reflect different steps in the processing of the image and thus visualize our processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHDf-Bq90CIn"
      },
      "source": [
        "class ImageShower(object):\n",
        "    \"\"\"Creates another TKInter Window and shows the given Image\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name=\"Window\", window=None):\n",
        "        \"\"\"\n",
        "        Initialize a new ImageShower, by creating another TKInter Window and set its Name\n",
        "        :param name:\n",
        "        \"\"\"\n",
        "        if window is None:\n",
        "            self.window = tk.Toplevel(app)\n",
        "            self.window.title(name)\n",
        "        else:\n",
        "            self.window = window\n",
        "\n",
        "        self.panel = None\n",
        "        self.frame = None\n",
        "\n",
        "    def update(self, image):\n",
        "        \"\"\"\n",
        "        Update the Image witch will be shown in this Window\n",
        "        :param image: The Image as cv2 Image in BGR\n",
        "        \"\"\"\n",
        "        self.frame = image\n",
        "\n",
        "    def show(self, width=640, height=360):\n",
        "        \"\"\"\n",
        "        Shows the Image, witch has been already set by the Update Method or is given by an Optional Parameter\n",
        "        :param frame: The Optional cv2 Image in BGR\n",
        "        :param width: The Optional scaled Width of the Image\n",
        "        :param height: The Optional scaled Height of the Image\n",
        "        :return: None if no Image is given\n",
        "        \"\"\"\n",
        "        if self.frame is None:\n",
        "            return\n",
        "        try:\n",
        "            # Resize and Convert cv2 Image to TKInter Image\n",
        "            img = cv2.resize(np.array(self.frame), (width, height), interpolation=cv2.INTER_AREA)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\n",
        "            img = Image.fromarray(img)\n",
        "            img = ImageTk.PhotoImage(img)\n",
        "            # if the panel is not None, we need to initialize it\n",
        "            if self.panel is None:\n",
        "                self.panel = tk.Label(self.window, image=img)\n",
        "                self.panel.image = img\n",
        "                self.panel.pack(side=tk.TOP)\n",
        "\n",
        "            # otherwise, simply update the panel\n",
        "            else:\n",
        "                self.panel.configure(image=img)\n",
        "                self.panel.image = img\n",
        "        except RuntimeError:\n",
        "            print(\"[INFO] caught a RuntimeError\")\n",
        "        except cv2.error:\n",
        "            print(\"[DEBUG] Bildfehler! (Format richtig?)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5CpfVtyjtf-"
      },
      "source": [
        "## Showing Windows\n",
        "The first step was to display the current monitor within the software. For this purpose we used the Python library 'mss'. It is able to read all connected monitors and to display data like the current screen content or the dimensions of the selected monitor. For debugging reasons, we also wanted to output individual intermediate steps, as well as various metadata, on the basis of which certain actions are performed.\n",
        "\n",
        "For this we could then also use the ImageShower shown earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmI3_82m0zN_"
      },
      "source": [
        "# Create Optional Windows for Debugging and Additional Infos\n",
        "histogramWindow = ImageShower(\"Histogram\")\n",
        "histogramThreshWindow = ImageShower(\"Histogram mit Threshhold\")\n",
        "mainCameraWithInfo = ImageShower(\"Hauptkamera mit Infos\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXG7TCukjvuw"
      },
      "source": [
        "### Live Camerafeed with generated metadata\n",
        "In order to find out whether our software is working correctly, the frame read in by the camera was output from a small extra window. This also contains further data such as an activation circle, the last recognized positions of the finger, as well as the assumed position of the back of the hand. Also available in the view is the current zoom level.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fZUzUw9jyZf"
      },
      "source": [
        "### Processed Image with Backprojection\n",
        "Another output represents a specific point in the actual image processing. After a histogram has been recorded, it is applied to the current camera image using backprojection. The result is all pixels that match parts of the histogram. All other parts of the image are black. This display was important to us because it provided important information about the processing steps that had already been performed. Also, whether various changes in the size of the histogram or in the parameters of the backprojection produced more positive results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVsOubqzj0Q6"
      },
      "source": [
        "### Processed Image with additional Thresholding\n",
        "Another processing step we used for debugging purposes was a small window showing the processed camera image after the additional thresholding. Here, too, various previously performed processing steps played a major role in the final quality. An example would be different lighting conditions, or different skin tones on the back and palm of the hand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOB8z0rtj3Ef"
      },
      "source": [
        "### Main-Window (Screen + PiP)\n",
        "To bring all the processing steps together, there is a main window. This contains both the possibility of the choice between different monitors and cameras, as well as the display of the selected monitor and the processed picture of the camera. The camera image is then only displayed when a finger is in the image. In addition, the image can be zoomed in or out using the aforementioned gesture recognition. The zoomed image always follows the finger and zooms to the displayed position. The zoom level is maintained even if the finger leaves the picture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNJm6kDAnA_5"
      },
      "source": [
        "## Performance Improvements\n",
        "One problem that came to our attention quite quickly was the performance drop after not only the current monitor was displayed in the window, but also the incoming camera image was processed. The problem with our software was that all actions happened on one thread. Both the reading of the monitor, the camera, as well as the entire processing and the subsequent display of the results. We came up with the idea that some sections of the program could be outsourced to separate threads in order to already read in the image that was to be processed and make it available by means of a variable. For this reason we have programmed two different program sections, which separately take care of the camera to be read in as well as the reading of the monitor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbINdPXOzC2J"
      },
      "source": [
        "class MonitorGrabber(object):\n",
        "    \"\"\"\n",
        "    Reads the Current Screen in another Thread and Stores it for easy Access\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, src=1, width=1280, height=720):\n",
        "        \"\"\"\n",
        "        Initialize a new MonitorGrabber\n",
        "        :param src: MonitorIndex from mss\n",
        "        :param width: Scaled Output Image width\n",
        "        :param height: Scaled Output Image hight\n",
        "        \"\"\"\n",
        "        self.setSrc(src)\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "        # Grab Monitor Image, Resize, Convert and Store it\n",
        "        img = sct.grab(self.src)\n",
        "        # noinspection PyTypeChecker\n",
        "        img = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        self.picture = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "        self.stopped = False\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"\n",
        "        Starts another Thread for its own get-Method, to grab the Image out of Mainloop\n",
        "        :return:  Optional: The Own Object to create, start the Thread and save the Object at the same Time\n",
        "        \"\"\"\n",
        "        Thread(target=self.get, args=()).start()\n",
        "        return self\n",
        "\n",
        "    def setSrc(self, src):\n",
        "        \"\"\"\n",
        "        Re-Sets the Monitor Input Source Index of mss\n",
        "        :param src: The new Monitor Index\n",
        "        \"\"\"\n",
        "        self.src = sct.monitors[src]\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Grabs the current Monitor Image, Resize, convert and stores it\n",
        "        \"\"\"\n",
        "        while not self.stopped:\n",
        "            img = sct.grab(self.src)\n",
        "            img = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "            self.picture = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"\n",
        "        Stops the MonitorGrabber-Get-Thread started by the start-Method\n",
        "        \"\"\"\n",
        "        self.stopped = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZsRj2Y_zHKy"
      },
      "source": [
        "class CameraGrabber(object):\n",
        "    \"\"\"\n",
        "    Reads the Current Camera-feed in another Thread and Stores it for easy Access\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, src, width=1280, height=720):\n",
        "        \"\"\"\n",
        "        Initialize a new CameraGrabber\n",
        "        :param src: CameraIndex from mss\n",
        "        :param width: Scaled Output Image width\n",
        "        :param height: Scaled Output Image hight\n",
        "        \"\"\"\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "        # Grab Camera Image, Resize, Convert and Store it\n",
        "        self.stream = cv2.VideoCapture(src)\n",
        "        (self.grabbed, img) = self.stream.read()\n",
        "        self.picture = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        self.stopped = False\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"\n",
        "        Starts another Thread for its own get-Method, to grab the Image out of Mainloop\n",
        "        :return:  Optional: The Own Object to create, start the Thread and save the Object at the same Time\n",
        "        \"\"\"\n",
        "        Thread(target=self.get, args=()).start()\n",
        "        return self\n",
        "\n",
        "    def setSrc(self, src):\n",
        "        \"\"\"\n",
        "        Re-Sets the Camera Input Source Index of mss\n",
        "        :param src: The new Camera Index\n",
        "        \"\"\"\n",
        "        self.stream = cv2.VideoCapture(src)\n",
        "\n",
        "    def get(self):\n",
        "        \"\"\"\n",
        "        Grabs the current Camera Image, Resize and stores it\n",
        "        \"\"\"\n",
        "        while not self.stopped:\n",
        "            if not self.grabbed:\n",
        "                self.stop()\n",
        "            else:\n",
        "                (self.grabbed, img) = self.stream.read()\n",
        "                self.picture = cv2.resize(np.array(img), (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"\n",
        "        Stops the CameraGrabber-Get-Thread started by the start-Method\n",
        "        \"\"\"\n",
        "        self.stopped = True"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}